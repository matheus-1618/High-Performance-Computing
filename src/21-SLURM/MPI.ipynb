{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Práticas com MPI & Cluster\n",
        "\n",
        "## Submeter o “Hello, World!” de MPI no cluster com slurm"
      ],
      "metadata": {
        "id": "JCwmaezfH0Od"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EXE8tdjp4ek"
      },
      "outputs": [],
      "source": [
        "%%writefile hello.cpp\n",
        "#include <mpi.h>\n",
        "#include <iostream>\n",
        "#include<unistd.h>\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "    MPI_Init(&argc, &argv);\n",
        "\n",
        "    int rank, size;\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
        "\n",
        "    if (size != 2) {\n",
        "        std::cerr << \"Este programa requer exatamente 2 processos.\" << std::endl;\n",
        "        MPI_Abort(MPI_COMM_WORLD, 1);\n",
        "    }\n",
        "\n",
        "    int message;\n",
        "    char hostname[256];\n",
        "\t  gethostname(hostname, sizeof(hostname));\n",
        "\n",
        "\n",
        "    if (rank == 0) {\n",
        "\n",
        "        message = 42;\n",
        "        MPI_Send(&message, 1, MPI_INT, 1, 0, MPI_COMM_WORLD);\n",
        "        std::cout << hostname << \": Processo \" << rank << \" enviou \" << message << \" para P1.\" << std::endl;\n",
        "\n",
        "\n",
        "        MPI_Recv(&message, 1, MPI_INT, 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
        "        std::cout << hostname << \": Processo \" << rank << \" recebeu \" << message << \" de P1.\" << std::endl;\n",
        "    } else {\n",
        "        MPI_Recv(&message, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
        "        std::cout << hostname << \": Processo \" << rank << \" recebeu \" << message << \" de P0.\" << std::endl;\n",
        "\n",
        "        message = 84;\n",
        "        MPI_Send(&message, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n",
        "        std::cout << hostname << \": Processo \" << rank << \" enviou \" << message << \" para P0.\" << std::endl;\n",
        "    }\n",
        "\n",
        "    MPI_Finalize();\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Arquivo do script slurm. Dependendo de como você estiver rodando, precisará usar o comando \"--allow-run-as-root\"."
      ],
      "metadata": {
        "id": "1jZ67RHFJE20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile hello.slurm\n",
        "#!/bin/bash\n",
        "#SBATCH -N 2\n",
        "#SBATCH --time=00:10:00\n",
        "#SBATCH --partition=normal\n",
        "\n",
        "mpirun --allow-run-as-root ./hello"
      ],
      "metadata": {
        "id": "MdH3igJIH_cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compilação do código"
      ],
      "metadata": {
        "id": "bG6XFBIaJ5Fz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mpic++ -o hello hello.cpp"
      ],
      "metadata": {
        "id": "gIQda8J-J6sL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Submetendo o job no cluster"
      ],
      "metadata": {
        "id": "uj8lAbtkJ9lg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sbatch hello.slurm"
      ],
      "metadata": {
        "id": "L9QuOdX_J_9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submeter um job de MPI que sobrecarrega a comunicação no cluster, observando o uso da rede no Ganglia\n"
      ],
      "metadata": {
        "id": "qJlrl8PoMXmB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile broadcast.cpp\n",
        "#include <mpi.h>\n",
        "#include <iostream>\n",
        "#include <string>\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "    MPI_Init(&argc, &argv);\n",
        "\n",
        "    int rank, size;\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
        "\n",
        "    const int iterations = 10000000;\n",
        "\n",
        "    for (int iter = 0; iter < iterations; ++iter) {\n",
        "        if (rank == 0) {\n",
        "\n",
        "            std::string message = \"Iteration \" + std::to_string(iter);;\n",
        "            MPI_Bcast(&message, 20, MPI_CHAR, 0, MPI_COMM_WORLD);\n",
        "            std::cout << \"Iteração \" << iter << \": Processo \" << rank << \" enviou \\\"\" << message << \"\\\" para todos os outros.\" << std::endl;\n",
        "        } else {\n",
        "\n",
        "            char received_message[20];\n",
        "            MPI_Bcast(&received_message, 20, MPI_CHAR, 0, MPI_COMM_WORLD);\n",
        "            std::cout << \"Iteração \" << iter << \": Processo \" << rank << \" recebeu \\\"\" << received_message << \"\\\" de P0.\" << std::endl;\n",
        "        }\n",
        "\n",
        "\n",
        "        MPI_Barrier(MPI_COMM_WORLD);\n",
        "    }\n",
        "\n",
        "    MPI_Finalize();\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "ePYkBa3DMfeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compilando o código."
      ],
      "metadata": {
        "id": "V4J76yUCJ5Zf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mpic++ -o broadcast broadcast.cpp"
      ],
      "metadata": {
        "id": "W0M9zFB9J4P6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Arquivo de configuração do slurm."
      ],
      "metadata": {
        "id": "fVWrXxFeNe2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile broadcast.slurm\n",
        "#!/bin/bash\n",
        "#SBATCH -n 2\n",
        "#SBATCH --time=00:10:00\n",
        "#SBATCH --partition=normal\n",
        "\n",
        "# Executa o código MPI\n",
        "mpirun --allow-run-as-root -n 2 ./broadcast"
      ],
      "metadata": {
        "id": "-lzyn6lANhrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Submissão do job"
      ],
      "metadata": {
        "id": "oS1qLIA-Njlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sbatch broadcast.slurm"
      ],
      "metadata": {
        "id": "DQ2WAaU8NklY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Round\n",
        "\n",
        "Altere a configuração do seu cluster para ter mais cores antes de rodar o código abaixo! Qual é a saída obtida?"
      ],
      "metadata": {
        "id": "WbMIQsYDvJgb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile round.cpp\n",
        "#include <mpi.h>\n",
        "#include <sys/time.h>\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "#define N (1024 * 1024 * 1)\n",
        "\n",
        "int main(int argc, char* argv[])\n",
        "{\n",
        "  int size, rank;\n",
        "  struct timeval start, end;\n",
        "  char hostname[256];\n",
        "  int hostname_len;\n",
        "\n",
        "  MPI_Init(&argc, &argv);\n",
        "\n",
        "  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "  MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
        "  MPI_Get_processor_name(hostname, &hostname_len);\n",
        "\n",
        "  char* buffer = (char*)malloc(sizeof(char) * N);\n",
        "\n",
        "\n",
        "  if (rank == 0) {\n",
        "    gettimeofday(&start, NULL);\n",
        "    printf(\"Rank %d (running on '%s'): sending the message rank %d\\n\", rank, hostname, 1);\n",
        "    MPI_Send(buffer, N, MPI_BYTE, 1, 1, MPI_COMM_WORLD);\n",
        "    MPI_Recv(buffer, N, MPI_BYTE, size - 1, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
        "    printf(\"Rank %d (running on '%s'): received the message from rank %d\\n\", rank, hostname, size - 1);\n",
        "    gettimeofday(&end, NULL);\n",
        "    printf(\"%f\\n\", (end.tv_sec * 1000000.0 + end.tv_usec - start.tv_sec * 1000000.0 - start.tv_usec) / 1000000.0);\n",
        "\n",
        "  } else {\n",
        "    MPI_Recv(buffer, N, MPI_BYTE, rank - 1, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
        "    printf(\"Rank %d (running on '%s'): receive the message and sending it to rank %d\\n\", rank, hostname,\n",
        "           (rank + 1) % size);\n",
        "    MPI_Send(buffer, N, MPI_BYTE, (rank + 1) % size, 1, MPI_COMM_WORLD);\n",
        "  }\n",
        "\n",
        "  MPI_Finalize();\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "4135KAsRvV0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora você já sabe como compilar e executar o código!\n",
        "\n",
        "## Soma de vetores\n",
        "\n",
        "1. Crie um programa via C++/MPI que calcula a soma de um vetor de 10k números.\n",
        "2. Submeta este programa no cluster via slurm:\n",
        " - Faça uma execução local (usando apenas um node)\n",
        " - Faça uma execução global (usando todos os nodes e recursos disponíveis).\n",
        " - Observe e compare os tempos de execução\n"
      ],
      "metadata": {
        "id": "0_eGQPSTveyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#include <mpi.h>\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <random>\n",
        "\n",
        "// size of array\n",
        "#define n 10000\n",
        "\n",
        "// Temporary array for slave process\n",
        "int a2[1000];\n",
        "\n",
        "int main(int argc, char *argv[]) {\n",
        "    int pid, np,\n",
        "        elements_per_process,\n",
        "        n_elements_received;\n",
        "    // np -> no. of processes\n",
        "    // pid -> process id\n",
        "\n",
        "    MPI_Status status;\n",
        "\n",
        "    // Initialize MPI\n",
        "    MPI_Init(&argc, &argv);\n",
        "\n",
        "    // find out process ID,\n",
        "    // and how many processes were started\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &pid);\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &np);\n",
        "\n",
        "    // master process\n",
        "    if (pid == 0) {\n",
        "        int index, i;\n",
        "        elements_per_process = n / np;\n",
        "\n",
        "        // Initialize vector with a for loop\n",
        "        std::vector<int> a;\n",
        "        for (int i = 0; i < n; ++i) {\n",
        "            a.push_back(i + 1); // Assuming you want elements from 1 to 10000\n",
        "        }\n",
        "\n",
        "        // check if more than 1 process is run\n",
        "        if (np > 1) {\n",
        "            // distributes the portion of array\n",
        "            // to child processes to calculate\n",
        "            // their partial sums\n",
        "            for (i = 1; i < np - 1; i++) {\n",
        "                index = i * elements_per_process;\n",
        "\n",
        "                MPI_Send(&elements_per_process,\n",
        "                         1, MPI_INT, i, 0,\n",
        "                         MPI_COMM_WORLD);\n",
        "                MPI_Send(&a[index],\n",
        "                         elements_per_process,\n",
        "                         MPI_INT, i, 0,\n",
        "                         MPI_COMM_WORLD);\n",
        "            }\n",
        "\n",
        "            // last process adds remaining elements\n",
        "            index = i * elements_per_process;\n",
        "            int elements_left = n - index;\n",
        "\n",
        "            MPI_Send(&elements_left,\n",
        "                     1, MPI_INT,\n",
        "                     i, 0,\n",
        "                     MPI_COMM_WORLD);\n",
        "            MPI_Send(&a[index],\n",
        "                     elements_left,\n",
        "                     MPI_INT, i, 0,\n",
        "                     MPI_COMM_WORLD);\n",
        "        }\n",
        "\n",
        "        // master process adds its own sub-array\n",
        "        int sum = 0;\n",
        "        for (i = 0; i < elements_per_process; i++)\n",
        "            sum += a[i];\n",
        "\n",
        "        // collects partial sums from other processes\n",
        "        int tmp;\n",
        "        for (i = 1; i < np; i++) {\n",
        "            MPI_Recv(&tmp, 1, MPI_INT,\n",
        "                     MPI_ANY_SOURCE, 0,\n",
        "                     MPI_COMM_WORLD,\n",
        "                     &status);\n",
        "            int sender = status.MPI_SOURCE;\n",
        "\n",
        "            sum += tmp;\n",
        "        }\n",
        "\n",
        "        // prints the final sum of array\n",
        "        std::cout << \"Sum of array is: \" << sum << std::endl;\n",
        "    }\n",
        "    // slave processes\n",
        "    else {\n",
        "        MPI_Recv(&n_elements_received,\n",
        "                 1, MPI_INT, 0, 0,\n",
        "                 MPI_COMM_WORLD,\n",
        "                 &status);\n",
        "\n",
        "        // stores the received array segment\n",
        "        // in the local array a2\n",
        "        MPI_Recv(&a2, n_elements_received,\n",
        "                 MPI_INT, 0, 0,\n",
        "                 MPI_COMM_WORLD,\n",
        "                 &status);\n",
        "\n",
        "        // calculates its partial sum\n",
        "        int partial_sum = 0;\n",
        "        for (int i = 0; i < n_elements_received; i++)\n",
        "            partial_sum += a2[i];\n",
        "\n",
        "        // sends the partial sum to the root process\n",
        "        MPI_Send(&partial_sum, 1, MPI_INT,\n",
        "                 0, 0, MPI_COMM_WORLD);\n",
        "    }\n",
        "\n",
        "    // cleans up all MPI state before exit of process\n",
        "    MPI_Finalize();\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "-hZKayuyv7Lp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}